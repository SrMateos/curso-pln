{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPTnXlyOM2VRT2pQG69cFV3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cbadenes/curso-pln/blob/main/notebooks/proyecto_apoyo/Keywords.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MODELO PARA EXTRAER KEYWORDS"
      ],
      "metadata": {
        "id": "q5j9sIAExXP7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "EmD_pSvyJeyE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "def cargar_modelo(model_name=MODEL_NAME):\n",
        "    \"\"\"\n",
        "    Carga el modelo TinyLlama.\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    return tokenizer, model\n",
        "\n",
        "def extraer_keywords(texto, tokenizer, model):\n",
        "    \"\"\"\n",
        "    Dado un texto, construye un 'prompt' que pide al modelo extraer palabras clave.\n",
        "    \"\"\"\n",
        "    prompt = (\"Eres un extractor de palabras clave. Tu tarea es la siguiente:\\n\\n\"\n",
        "        \"1. Lee el texto a continuación.\\n\"\n",
        "        \"2. Devuelve únicamente las TRES palabras clave más importantes.\\n\"\n",
        "        \"3. No incluyas explicaciones ni texto adicional.\\n\"\n",
        "        \"4. Sepáralas por comas y nada más.\\n\\n\"\n",
        "        f\"Texto:\\n{texto}\\n\\n\"\n",
        "        \"Palabras clave:\"\n",
        "    )\n",
        "\n",
        "    # Tokenizamos el prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    model.eval()\n",
        "    # Generamos la respuesta\n",
        "    with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=32,\n",
        "            temperature=0.1,   # Al ser una tarea \"extractiva\", conviene un valor bajo\n",
        "            do_sample=False,   # Modo determinista\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Decodificamos el texto generado\n",
        "    respuesta_completa = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extraemos lo que viene tras \"Palabras clave:\"\n",
        "    # Nota: Esto asume que el modelo va a contestar siguiendo el prompt.\n",
        "    partes = respuesta_completa.split(\"Palabras clave:\")\n",
        "    if len(partes) > 1:\n",
        "        # Tomamos la última parte (el contenido tras \"Palabras clave:\")\n",
        "        keywords = partes[-1].strip()\n",
        "    else:\n",
        "        # Si no encuentra el marcador, devolvemos todo el texto\n",
        "        keywords = respuesta_completa\n",
        "\n",
        "    return keywords\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Cargamos el modelo y el tokenizer\n",
        "    tokenizer, model = cargar_modelo()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FUNCIÓN PARA POST-PROCESAR LA SALIDA DEL MODELO"
      ],
      "metadata": {
        "id": "SqO6Avq0xRga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def postprocesar_keywords(raw_keywords: str) -> str:\n",
        "    \"\"\"\n",
        "    Toma la cadena de salida del modelo y devuelve hasta tres palabras\n",
        "    clave, separadas por comas, eliminando duplicados y texto sobrante.\n",
        "    \"\"\"\n",
        "    # 1. Dividimos por comas o saltos de línea\n",
        "    #    (si el modelo genera varias palabras en línea separadas por comas o saltos).\n",
        "    tokens = re.split(r\"[,\\n]+\", raw_keywords)\n",
        "\n",
        "    # 2. Limpiamos espacios y descartamos vacíos.\n",
        "    tokens = [t.strip() for t in tokens if t.strip()]\n",
        "\n",
        "    # 3. (Opcional) Eliminar duplicados manteniendo el orden.\n",
        "    sin_duplicados = []\n",
        "    seen = set()\n",
        "    for token in tokens:\n",
        "        if token.lower() not in seen:\n",
        "            sin_duplicados.append(token)\n",
        "            seen.add(token.lower())\n",
        "\n",
        "    # 4. Quedarnos solo con los primeros 3.\n",
        "    top3 = sin_duplicados[:3]\n",
        "\n",
        "    # 5. Volver a unir en una cadena, separada por comas.\n",
        "    return \", \".join(top3)\n"
      ],
      "metadata": {
        "id": "TEultVu3vva9"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EJEMPLOS"
      ],
      "metadata": {
        "id": "3At3Rg1CwZ_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # Texto de ejemplo\n",
        "    texto_entrada = (\n",
        "        \"¿Qué sabes con respecto a las normativas relacionadas con la discapacidad?\"\n",
        "    )\n",
        "\n",
        "    # Extraemos las palabras clave\n",
        "    resultado = extraer_keywords(texto_entrada, tokenizer, model)\n",
        "    # Post-procesamos para asegurar un formato limpio\n",
        "    resultado = postprocesar_keywords(resultado)\n",
        "    print(\"Palabras clave extraídas:\", resultado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NViu09s0wbmv",
        "outputId": "cca8d0e7-3761-45c1-a0f4-656e05b50933"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras clave extraídas: 1. Discapacidad, 2. Normativas, 3. Discapacidad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Texto de ejemplo\n",
        "    texto_entrada = (\n",
        "        \"¿Dónde puedo encontrar subvenciones agrarias para jóvenes agricultores?\"\n",
        "    )\n",
        "    # Extraemos las palabras clave\n",
        "    resultado = extraer_keywords(texto_entrada, tokenizer, model)\n",
        "    # Post-procesamos para asegurar un formato limpio\n",
        "    resultado = postprocesar_keywords(resultado)\n",
        "    print(\"Palabras clave extraídas:\", resultado)"
      ],
      "metadata": {
        "id": "uvK2FlzXQrMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40faccdb-93cf-4626-ecc8-57a8cbf05c93"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras clave extraídas: agricultura, jóvenes, subvenciones\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Texto de ejemplo\n",
        "    texto_entrada = (\n",
        "        \"Necesito información sobre la ley de protección de datos en empresas tecnológicas.\"\n",
        "    )\n",
        "    # Extraemos las palabras clave\n",
        "    resultado = extraer_keywords(texto_entrada, tokenizer, model)\n",
        "    # Post-procesamos para asegurar un formato limpio\n",
        "    resultado = postprocesar_keywords(resultado)\n",
        "    print(\"Palabras clave extraídas:\", resultado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMupR1sFwkzO",
        "outputId": "fdd5e121-b93a-461a-b26a-a9777e58c7d6"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras clave extraídas: protección, datos, tecnología\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Texto de ejemplo\n",
        "    texto_entrada = (\n",
        "        \"Requiero documentación acerca de la normativa ambiental que prohíba vertidos al río.\"\n",
        "    )\n",
        "    # Extraemos las palabras clave\n",
        "    resultado = extraer_keywords(texto_entrada, tokenizer, model)\n",
        "    # Post-procesamos para asegurar un formato limpio\n",
        "    resultado = postprocesar_keywords(resultado)\n",
        "    print(\"Palabras clave extraídas:\", resultado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpwhZ5JHwlac",
        "outputId": "751549be-81d4-4113-aec8-99e7eda40a97"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras clave extraídas: Normativa, ambiental, vertidos\n"
          ]
        }
      ]
    }
  ]
}