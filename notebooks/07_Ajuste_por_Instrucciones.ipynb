{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cbadenes/curso-pln/blob/main/notebooks/07_Ajuste_por_Instrucciones.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_x_vTlpjBy56"
   },
   "source": [
    "# Instruction Tuning: Una Guía Práctica\n",
    "\n",
    "En este notebook aprenderemos sobre el ajuste por instrucciones (instruction tuning) de una manera práctica y sencilla.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95lD5hH4Cdbk"
   },
   "source": [
    "## 1) Configuración del Entorno\n",
    " Primero, importamos las bibliotecas necesarias y configuramos nuestro entorno.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8SeusDuV7f3A",
    "outputId": "7c9b48ff-d316-4fe2-97ba-91c7740781f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instalando bibliotecas necesarias...\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mDispositivo disponible: cpu\n"
     ]
    }
   ],
   "source": [
    "# Instalamos las bibliotecas necesarias\n",
    "print(\"Instalando bibliotecas necesarias...\")\n",
    "!pip install --quiet transformers datasets torch seqeval\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Verificamos si tenemos GPU disponible\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Dispositivo disponible: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hzT8O_UCil8"
   },
   "source": [
    "## 2) Preparación de Datos\n",
    "Creamos un conjunto simple de datos de ejemplo con instrucciones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kSHx3E3QCm-Y",
    "outputId": "2c4643c9-0a20-4e8f-bdeb-7ebb4f5f6a65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo del dataset:\n",
      "{'instruccion': 'Explica qué es una estrella', 'contexto': '', 'respuesta': 'Una estrella es un cuerpo celeste masivo que produce luz y calor mediante reacciones nucleares en su núcleo. El Sol es un ejemplo de estrella.'}\n"
     ]
    }
   ],
   "source": [
    "instrucciones = [\n",
    "    {\n",
    "        \"instruccion\": \"Explica qué es una estrella\",\n",
    "        \"contexto\": \"\",\n",
    "        \"respuesta\": \"Una estrella es un cuerpo celeste masivo que produce luz y calor mediante reacciones nucleares en su núcleo. El Sol es un ejemplo de estrella.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruccion\": \"Define qué es un planeta\",\n",
    "        \"contexto\": \"\",\n",
    "        \"respuesta\": \"Un planeta es un cuerpo celeste que orbita alrededor de una estrella, tiene suficiente masa para ser esférico y ha limpiado su órbita de otros objetos.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruccion\": \"Explica qué es la fotosíntesis\",\n",
    "        \"contexto\": \"\",\n",
    "        \"respuesta\": \"La fotosíntesis es el proceso por el cual las plantas convierten la luz solar en energía química, produciendo oxígeno y glucosa a partir de agua y dióxido de carbono.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruccion\": \"Describe qué es un átomo\",\n",
    "        \"contexto\": \"\",\n",
    "        \"respuesta\": \"Un átomo es la unidad más pequeña de la materia que mantiene las propiedades de un elemento químico. Está compuesto por un núcleo con protones y neutrones, rodeado por electrones.\"\n",
    "    },\n",
    "    {\n",
    "        \"instruccion\": \"Explica qué es la gravedad\",\n",
    "        \"contexto\": \"\",\n",
    "        \"respuesta\": \"La gravedad es una fuerza fundamental de la naturaleza que hace que los objetos con masa se atraigan entre sí. Es la fuerza que nos mantiene en la Tierra y hace que los planetas orbiten alrededor del Sol.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convertimos a Dataset de HuggingFace\n",
    "dataset = Dataset.from_list(instrucciones)\n",
    "\n",
    "# Mostramos un ejemplo\n",
    "print(\"Ejemplo del dataset:\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVnJJzAOC9g4"
   },
   "source": [
    "## 3) Carga del Modelo Base\n",
    " Utilizaremos un modelo pequeño para nuestras pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276,
     "referenced_widgets": [
      "4b3a89e891494b368f96d53dd8ee20e0",
      "4f7afaa2d4634db2acdc8a24098f41e8",
      "17668e4231f441d7922ddcb7a7c29451",
      "25b4386cd4a04dd19f3ea000f6dae0e2",
      "0412b895cdcc41c488eab7608e6d8f90",
      "2c34c34349984573a36050264c7f1969",
      "4e27be8a34a644bf94c5f957957200a7",
      "ad3a01326b76435cb361f1055016f3dd",
      "2458d6690ac54ecca068b17b6ff0bc56",
      "9af8e8fc421c47439f309ddf51c51fec",
      "7ff0a163fece4f8baa4081e86042c145",
      "88cf98e7415741b1b42efbd9be095b63",
      "0a37c487a26047c38102c08306600855",
      "b1fbe81b81e04a84b05dfc61df3e60cf",
      "e09afd2e00a84d87898ed25f2272e14f",
      "af765106d54f422f8d2d6114eaf1e8d7",
      "06c225e3a3a448399c9a1753fa04c5cf",
      "35fa1b20e33f4ced8cf487ddce3ba923",
      "5816ea85aabb4ac3bd3e12775e23f69a",
      "0854b283f7e2417dac0d4a4c9470676d",
      "aec226f4a0e249c294d5adba47a02e33",
      "96dbbe1f710d467f80885d2b027f2e6d",
      "a2e92aaff35c448c938b5566e2232c09",
      "53cfae6df7a44832a0058060a58c9f00",
      "7449c3c088394783bf03a18ea33c6678",
      "24e117bf4b1140a8a5df53cfc74eba3e",
      "2328163e5a0641a2afe3709c507f79f9",
      "a565db81cf044b76a965b1e51e85b9ee",
      "09a45aeb71e94477bc978462b98d0cea",
      "82de6b64fc60490c8ce1791541a28f20",
      "30fa9912c1144299857285dc3420979a",
      "11d1888ae07445aa9df4291bbfb7e319",
      "7f59a5fb0ea345b7a1938080285a0654",
      "17f9cc20174a4e8e9f2ea0914f6cace9",
      "aba2a4d5108d47658d55635f32c213ab",
      "303fac05d6d24982a89dbc705c550320",
      "5fd1b5afd0e44c0ba0a9cdf05f12fa96",
      "b0346cddbd4a44e7a922478d3c9a7d02",
      "6d15b45a5447427da6aa7b450d32ffac",
      "453d594da14e424c87bf999944c42f03",
      "9ad05a55ff72489e871f6e04dd49b1fb",
      "2ba66bfd9363454db8dbf16b9540a3aa",
      "b867b9f038b2432091cf18f079937b0e",
      "89056f0ea77e485b9e14fdf176644d50",
      "5a8da9acb94c4f0f9241cec2613b46cb",
      "cb8d6373f68f439da3e4c83449367c9a",
      "36bb3b27cb774a8d8deee6a14e249c32",
      "8b133886c3e64b718f8f53120d8c0af8",
      "88d3105d41004288a09a3f6d9c1c8312",
      "943b00be2b61416898f1869c87199dbe",
      "85ca2c0cb72c49fda0002a22464a17ba",
      "51b2b9f38ed94a9b826338e8e7adffd3",
      "4e86371938e3499e80ecd13aa39ef83b",
      "533eb8d355004d7f83792f4e8aaa6bcd",
      "84318afaa63f41bf8c23f5e386465f20",
      "f68b715ac1b34a87b77eefa918cd10ca",
      "b207c1924c764f78a6da47a42f85e57a",
      "934a0e5c90d14329997535fd3ebf64e1",
      "7e175d02d6ac4bb69b116a6e72ecbe9c",
      "36ff9e094a39461cb734ca8c15b0d3c0",
      "823e4bc3230843759149ee7beb53f7a9",
      "0e08a632ca3b41ef97839289884242f7",
      "c447b9852702485b93d3c1547966ec51",
      "3a57be967a054ab9923a9263348c488f",
      "7b8012d93d21499db090ecdd792a5b29",
      "6cabbd8f17404b6c88812ad96f4016bf",
      "35d3ab2d2a984a1bb93c2a7436cd9dfd",
      "4fa231f3418d433ebbd676d0fefbb2de",
      "c13730433e2349c2a33cea7f2c6bb709",
      "c6aa8c99910c4256a988cc61c4558f39",
      "f659d6eb8b154f748a11455dc451a58b",
      "c55b5de9c21642099f6f50a515fa2ade",
      "4dbd2d70e4bc43e48b841fa9024ba789",
      "deb7bfd2d5ba4e8d87acc48ffc3125b7",
      "7d5aeeb068bf41468c0b581bdd9b8fe5",
      "8f2ad97ef9604508b2df9663f79fbadd",
      "c4cedb2ba11a46199851767f4b9018af"
     ]
    },
    "id": "ypRrCtuuDCwz",
    "outputId": "192a06fd-8d6b-40b0-8244-c76da2949c68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo: facebook/opt-350m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3a89e891494b368f96d53dd8ee20e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88cf98e7415741b1b42efbd9be095b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e92aaff35c448c938b5566e2232c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f9cc20174a4e8e9f2ea0914f6cace9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8da9acb94c4f0f9241cec2613b46cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68b715ac1b34a87b77eefa918cd10ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d3ab2d2a984a1bb93c2a7436cd9dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo y tokenizer cargados correctamente\n"
     ]
    }
   ],
   "source": [
    "# Cargamos el modelo y el tokenizador\n",
    "modelo_nombre = \"gpt2\"  # Usamos GPT-2 por su tamaño reducido\n",
    "print(f\"Cargando modelo: {modelo_nombre}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelo_nombre)\n",
    "model = AutoModelForCausalLM.from_pretrained(modelo_nombre)\n",
    "\n",
    "# Configuración básica del tokenizer\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "print(\"Modelo y tokenizer cargados correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kA1kC7EyDHXE"
   },
   "source": [
    "## 4) Preparación de los Datos\n",
    "Formateamos nuestras instrucciones para el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xmEBkH1DJbd",
    "outputId": "70320337-af8f-4581-ae66-71d2834e7554"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo formateado:\n",
      "\n",
      "### Instrucción: Explica qué es una estrella\n",
      "### Contexto: \n",
      "### Respuesta: Una estrella es un cuerpo celeste masivo que produce luz y calor mediante reacciones nucleares en su núcleo. El Sol es un ejemplo de estrella.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def formatear_instruccion(ejemplo):\n",
    "    \"\"\"\n",
    "    Formatea cada ejemplo en un formato consistente\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "### Instrucción: {ejemplo['instruccion']}\n",
    "### Contexto: {ejemplo['contexto']}\n",
    "### Respuesta: {ejemplo['respuesta']}\n",
    "\"\"\"\n",
    "\n",
    "# Mostramos un ejemplo formateado\n",
    "ejemplo_formateado = formatear_instruccion(dataset[0])\n",
    "print(\"Ejemplo formateado:\")\n",
    "print(ejemplo_formateado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdhERADBDOtZ"
   },
   "source": [
    "## 5) Tokenización de Datos\n",
    "Convertimos nuestros textos en tokens que el modelo puede procesar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "f6cd1aac559f47dfa14b7079e6b29abf",
      "9317daae7bb14a4c861f9a52966780e2",
      "e454311fb5fd49f6b673337038497d71",
      "9635b4bc85074ed69dadab416230b41b",
      "6e67a11921a24b2bbfbafd97fc8fbee8",
      "9fe09a31b6864aa9b99fd893fb2cd5e8",
      "dd4c6482db9143cd95c9ee2bd8bde79c",
      "15acc38251854a6b9d4da9cc9d0fa1e9",
      "029d42be74e54af6afcb99f6c9eb1658",
      "8a5511bb97c84232ae1fb6c9ec8a7f4a",
      "1bf85b0cf20b489ea6b541240729c7b9"
     ]
    },
    "id": "oBOhkVp5DQw-",
    "outputId": "74399f35-e748-4d9f-ae6a-82b6dd5d7e1e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6cd1aac559f47dfa14b7079e6b29abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tokenizado correctamente\n"
     ]
    }
   ],
   "source": [
    "def tokenizar_datos(ejemplo):\n",
    "    \"\"\"\n",
    "    Tokeniza un ejemplo formateado y prepara los labels para el entrenamiento\n",
    "    \"\"\"\n",
    "    # Tokenizamos el texto completo\n",
    "    texto_completo = formatear_instruccion(ejemplo)\n",
    "    encodings = tokenizer(\n",
    "        texto_completo,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Los labels son los mismos que input_ids para entrenamiento de lenguaje\n",
    "    encodings['labels'] = encodings['input_ids'].copy()\n",
    "\n",
    "    return encodings\n",
    "\n",
    "# Tokenizamos el dataset\n",
    "dataset_tokenizado = dataset.map(tokenizar_datos)\n",
    "print(\"Dataset tokenizado correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWNC4zl7DUyE"
   },
   "source": [
    "## 6) Configuración del Entrenamiento\n",
    "Definimos los parámetros para el ajuste del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "VXawdsjsDWjJ"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args_entrenamiento = TrainingArguments(\n",
    "    output_dir=\"./modelo_ajustado\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=5,\n",
    "    report_to=\"none\",\n",
    "    eval_strategy=\"no\"  # No hacemos evaluación en este ejemplo simple\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1R7fzCMDb0A"
   },
   "source": [
    "## 7) Entrenamiento del Modelo\n",
    "\n",
    "Realizamos el ajuste del modelo con nuestros datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "Al_d00h5DjCQ",
    "outputId": "2c04e787-7660-4d2d-f5d5-2f1695c05ea0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comenzando el entrenamiento...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 06:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.104600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.698400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.301600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15, training_loss=1.3681646505991618, metrics={'train_runtime': 417.5949, 'train_samples_per_second': 0.06, 'train_steps_per_second': 0.036, 'total_flos': 5824472678400.0, 'train_loss': 1.3681646505991618, 'epoch': 5.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args_entrenamiento,\n",
    "    train_dataset=dataset_tokenizado,\n",
    ")\n",
    "\n",
    "# Iniciamos el entrenamiento\n",
    "print(\"Comenzando el entrenamiento...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37-EsgcPDlvN"
   },
   "source": [
    "## 8) Prueba del Modelo\n",
    "\n",
    "Probamos nuestro modelo ajustado con nuevas instrucciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dMLp_Yp6D0Nr",
    "outputId": "fe0f74f5-c630-47b0-cd42-7d17c556ae9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instrucción: Explica qué es una estrella\n",
      "Respuesta: Una estrellana es un cuerpo celeste masivo que produce un núcleo con una fuerza química. El Sol es un ejemplo de estrella.\n",
      "▄ {: [|\u0019 ================================= \"$:/aditional>>\\ Philips Hue lightens up to match the surrounding area's temperature and produces a white light that is emitted by a set of LEDs that are connected to a power source.\n"
     ]
    }
   ],
   "source": [
    "def probar_modelo(instruccion, contexto=\"\"):\n",
    "    \"\"\"\n",
    "    Prueba el modelo con una nueva instrucción\n",
    "    \"\"\"\n",
    "    # Formateamos el prompt incluyendo un marcador claro para la respuesta\n",
    "    prompt = f\"\"\"### Instrucción: {instruccion}\n",
    "### Contexto: {contexto}\n",
    "### Respuesta: La respuesta a esta instrucción es:\"\"\"\n",
    "\n",
    "    # Tokenizamos el prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Generamos la respuesta con parámetros más conservadores\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=150,          # Longitud máxima razonable\n",
    "        min_length=30,           # Forzamos una respuesta mínima\n",
    "        temperature=0.5,         # Temperatura más conservadora\n",
    "        do_sample=True,\n",
    "        top_p=0.85,\n",
    "        top_k=40,\n",
    "        no_repeat_ngram_size=3,  # Evitamos más repeticiones\n",
    "        num_beams=3,             # Añadimos beam search\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # Decodificamos y limpiamos la respuesta\n",
    "    respuesta_completa = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extraemos solo la parte después de nuestro marcador\n",
    "    try:\n",
    "        respuesta = respuesta_completa.split(\"La respuesta a esta instrucción es:\")[-1].strip()\n",
    "        return respuesta if respuesta else \"No se generó una respuesta válida.\"\n",
    "    except:\n",
    "        return \"Error al procesar la respuesta.\"\n",
    "\n",
    "# Probamos con una nueva instrucción\n",
    "nueva_instruccion = \"Explica qué es una estrella\"\n",
    "respuesta = probar_modelo(nueva_instruccion)\n",
    "print(f\"Instrucción: {nueva_instruccion}\")\n",
    "print(f\"Respuesta: {respuesta}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO16xScxRr/Zi7NCknY/vC7",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
